---
phase: 05-hetzner-deployment
plan: 01
type: execute
---

<objective>
Create all production deployment artifacts needed to deploy the collector daemon to a Hetzner CPX31 server: production Docker Compose, systemd service, bootstrap script, age-encrypted key management, and production config templates.

Purpose: Prepare everything locally so deployment to the server is a single clone + script run. No manual configuration assembly on the server.
Output: Complete `deploy/` directory with all infrastructure-as-code artifacts ready for Phase 5 Plan 2 (server provisioning + deployment).
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior phase summaries (dependency chain):
@.planning/phases/04-daemon-supervisor-cli/04-01-SUMMARY.md
@.planning/phases/04-daemon-supervisor-cli/04-02-SUMMARY.md

# Key source files for deployment:
@src/main.py
@src/config.py
@docker-compose.yml
@config.example.yaml
@requirements.txt
@src/collector/daemon.py

# Deployment research:
@docs/AWS_INFRASTRUCTURE.md

**Tech stack available:** asyncpg, TimescaleDB, Click CLI, asyncio daemon
**Established patterns:** docker-compose.yml for TimescaleDB, config.yaml + env vars for secrets, `python -m src.main collect` entry point
**Constraining decisions:**
- Hetzner CPX31 Frankfurt ($14/mo, 4 vCPU, 8GB RAM, 160GB NVMe)
- age-encryption for private key (not AWS Secrets Manager)
- Docker for TimescaleDB on production
- German IP = not geoblocked
- Dedicated wallet for bot, never accessed from US IP
</context>

<tasks>

<task type="auto">
  <name>Task 1: Production Docker Compose + config template</name>
  <files>deploy/docker-compose.prod.yml, config.prod.yaml</files>
  <action>
  Create `deploy/docker-compose.prod.yml` for production TimescaleDB:
  - Use env vars for credentials: `${POSTGRES_USER}`, `${POSTGRES_PASSWORD}` (NOT hardcoded dev passwords)
  - Add resource limits: `mem_limit: 2g`, `cpus: '1.0'` (leave headroom for Python app on 4 vCPU / 8GB server)
  - `restart: always` policy
  - Named volume `timescaledb_data` mounted to `/var/lib/postgresql/data`
  - Health check: `pg_isready -U ${POSTGRES_USER}` with 5s interval/timeout, 5 retries
  - `TIMESCALEDB_TELEMETRY: "off"`
  - Listen only on localhost: ports `127.0.0.1:5432:5432` (not exposed to internet)
  - Add `shm_size: 256mb` for TimescaleDB performance
  - Use pinned image tag `timescale/timescaledb:latest-pg17` (same as dev)

  Create `config.prod.yaml` as production config template:
  - database.url: `postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/polymarket` (note: env vars are resolved at runtime by the deploy script, not by YAML — use literal placeholder strings with comments)
  - collector intervals: same as dev defaults (price 60s, orderbook 300s, metadata 300s, resolution 300s)
  - logging.level: "INFO"
  - logging.file: "/opt/polymarket-stat-arb/logs/collector.log"
  - paper_trading: true (safe default — user changes when ready)
  - Include comments explaining each production-relevant setting

  Do NOT modify the existing dev docker-compose.yml — production is separate.
  </action>
  <verify>Both files are valid YAML: `python -c "import yaml; yaml.safe_load(open('deploy/docker-compose.prod.yml'))"; python -c "import yaml; yaml.safe_load(open('config.prod.yaml'))"` — both succeed without error</verify>
  <done>deploy/docker-compose.prod.yml exists with env-based credentials, resource limits, restart policy, localhost-only binding. config.prod.yaml exists with production paths and documented settings.</done>
</task>

<task type="auto">
  <name>Task 2: systemd service + deploy script</name>
  <files>deploy/polymarket-collector.service, deploy/deploy.sh</files>
  <action>
  Create `deploy/polymarket-collector.service` (systemd unit file):
  - `[Unit]` section: Description, After=docker.service network-online.target, Wants=network-online.target, Requires=docker.service
  - `[Service]` section:
    - Type=simple
    - User=polymarket (non-root service user)
    - WorkingDirectory=/opt/polymarket-stat-arb
    - EnvironmentFile=/opt/polymarket-stat-arb/.env.production
    - ExecStart=/opt/polymarket-stat-arb/venv/bin/python -m src.main collect
    - Restart=always
    - RestartSec=10
    - StandardOutput=journal
    - StandardError=journal
    - SyslogIdentifier=polymarket-collector
  - `[Install]` section: WantedBy=multi-user.target

  Create `deploy/deploy.sh` (server bootstrap script, run as root):
  - Shebang: `#!/usr/bin/env bash` with `set -euo pipefail`
  - Accept optional arg for repo path (default: /opt/polymarket-stat-arb)
  - Step 1: Install system deps (docker.io, docker-compose-plugin, python3, python3-venv, python3-pip, git, age)
    - Use `apt-get update && apt-get install -y` (Ubuntu/Debian — Hetzner default)
    - Enable and start Docker service
  - Step 2: Create `polymarket` system user (if not exists) with home /opt/polymarket-stat-arb
  - Step 3: Clone or update repo
    - If repo dir doesn't exist: `git clone` (URL left as placeholder — user fills in)
    - If exists: `git pull`
  - Step 4: Create Python venv and install deps
    - `python3 -m venv venv`
    - `venv/bin/pip install -r requirements.txt`
  - Step 5: Copy production config
    - Copy config.prod.yaml to config.yaml (if config.yaml doesn't exist)
    - Substitute env vars in config.yaml using sed (POSTGRES_USER, POSTGRES_PASSWORD from .env.production)
  - Step 6: Start TimescaleDB
    - `docker compose -f deploy/docker-compose.prod.yml up -d`
    - Wait for health check: loop with `docker compose -f deploy/docker-compose.prod.yml exec timescaledb pg_isready`
  - Step 7: Run database migrations
    - `sudo -u polymarket venv/bin/python -c "import asyncio; from src.db.pool import get_pool; from src.db.migrations.runner import run_migrations; from pathlib import Path; asyncio.run((lambda: None)())"` — actually use a small inline migration script
    - Better: create a `deploy/run-migrations.py` helper script that does pool init + migration
  - Step 8: Install systemd service
    - `cp deploy/polymarket-collector.service /etc/systemd/system/`
    - `systemctl daemon-reload`
    - `systemctl enable polymarket-collector`
  - Step 9: Create log directory
    - `mkdir -p /opt/polymarket-stat-arb/logs`
    - `chown -R polymarket:polymarket /opt/polymarket-stat-arb`
  - Print summary of what was done and next steps (start service, encrypt key)

  Also create `deploy/run-migrations.py`:
  - Simple asyncio script that initializes pool, runs migrations, closes pool
  - Uses the existing run_migrations() function

  Make deploy.sh executable: `chmod +x deploy/deploy.sh`

  IMPORTANT: deploy.sh should be idempotent — safe to run multiple times (check before creating user, check before cloning, etc.)
  </action>
  <verify>bash -n deploy/deploy.sh (syntax check passes), python -c "import ast; ast.parse(open('deploy/run-migrations.py').read())" (valid Python syntax), file deploy/polymarket-collector.service shows valid INI format</verify>
  <done>deploy/polymarket-collector.service is a complete systemd unit targeting the collector daemon. deploy/deploy.sh is an idempotent bootstrap script covering Docker, Python, venv, systemd installation. deploy/run-migrations.py can run migrations standalone.</done>
</task>

<task type="auto">
  <name>Task 3: age encryption scripts + environment template</name>
  <files>deploy/encrypt-key.sh, deploy/decrypt-key.sh, .env.production.example</files>
  <action>
  Create `deploy/encrypt-key.sh`:
  - Shebang with `set -euo pipefail`
  - Check `age` is installed: `command -v age >/dev/null || { echo "Install age: apt-get install age"; exit 1; }`
  - Accept private key as stdin or argument (never echo to terminal)
  - Generate age keypair if none exists: `age-keygen -o /opt/polymarket-stat-arb/.age-key.txt 2>/dev/null`
  - Extract public key from age key: `age-keygen -y /opt/polymarket-stat-arb/.age-key.txt`
  - Encrypt: pipe private key through `age -r <public-key> -o /opt/polymarket-stat-arb/.poly-key.age`
  - Set strict permissions: `chmod 600 .age-key.txt .poly-key.age`
  - Print success message with instructions
  - IMPORTANT: Never write the plaintext private key to disk — pipe directly from stdin

  Create `deploy/decrypt-key.sh`:
  - Shebang with `set -euo pipefail`
  - Decrypt age-encrypted key and export as env var: `export POLY_PRIVATE_KEY=$(age -d -i /opt/polymarket-stat-arb/.age-key.txt /opt/polymarket-stat-arb/.poly-key.age)`
  - This script is `source`d by the systemd ExecStartPre or by a wrapper
  - Alternative: modify the systemd service to use ExecStartPre to decrypt and pass via env

  Create `.env.production.example` at project root:
  - Document ALL required environment variables with comments:
    - `POSTGRES_USER=polymarket` (TimescaleDB username)
    - `POSTGRES_PASSWORD=` (CHANGE THIS — generate with `openssl rand -base64 32`)
    - `POLY_PRIVATE_KEY=` (Polymarket wallet private key — use age encryption instead of storing here)
    - `TELEGRAM_BOT_TOKEN=` (Optional — for alerts)
  - Include section headers and explanatory comments
  - Note which vars are required vs optional
  - Note that POLY_PRIVATE_KEY should ideally come from age decryption, not this file

  Make scripts executable: `chmod +x deploy/encrypt-key.sh deploy/decrypt-key.sh`

  Update the systemd service in mind: the service should use ExecStartPre to decrypt the key.
  Actually, better approach: update deploy/polymarket-collector.service to add:
  - `ExecStartPre=/bin/bash -c 'export POLY_PRIVATE_KEY=$(/usr/bin/age -d -i /opt/polymarket-stat-arb/.age-key.txt /opt/polymarket-stat-arb/.poly-key.age)'`
  Wait — ExecStartPre env vars don't carry over. Better: create a wrapper script `deploy/start-collector.sh` that decrypts the key, exports it, then exec's the Python process. The systemd ExecStart calls this wrapper.

  Create `deploy/start-collector.sh`:
  - Decrypt key: `export POLY_PRIVATE_KEY=$(age -d -i /opt/polymarket-stat-arb/.age-key.txt /opt/polymarket-stat-arb/.poly-key.age)`
  - Source .env.production for other vars (POSTGRES_USER, POSTGRES_PASSWORD, etc.)
  - Exec the collector: `exec /opt/polymarket-stat-arb/venv/bin/python -m src.main collect`
  - The exec replaces the shell process so systemd tracks the Python PID directly

  Then update the systemd service ExecStart to: `/opt/polymarket-stat-arb/deploy/start-collector.sh`
  </action>
  <verify>bash -n deploy/encrypt-key.sh && bash -n deploy/decrypt-key.sh && bash -n deploy/start-collector.sh (all syntax-valid). .env.production.example exists with documented variables.</verify>
  <done>deploy/encrypt-key.sh encrypts private key with age. deploy/decrypt-key.sh decrypts at runtime. deploy/start-collector.sh is the systemd entry point that decrypts key + launches daemon. .env.production.example documents all required env vars with instructions.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `deploy/` directory exists with all 7 files (docker-compose.prod.yml, polymarket-collector.service, deploy.sh, run-migrations.py, encrypt-key.sh, decrypt-key.sh, start-collector.sh)
- [ ] config.prod.yaml exists at project root with production settings
- [ ] .env.production.example exists at project root with documented env vars
- [ ] All .sh scripts have valid bash syntax: `for f in deploy/*.sh; do bash -n "$f"; done`
- [ ] All .py files have valid syntax: `python -c "import ast; ast.parse(open('deploy/run-migrations.py').read())"`
- [ ] docker-compose.prod.yml is valid YAML
- [ ] systemd service file references correct paths and user
</verification>

<success_criteria>

- All deployment artifacts created locally
- Scripts are idempotent and documented
- No hardcoded secrets anywhere (all via env vars or age encryption)
- Production Docker Compose binds to localhost only (not exposed)
- systemd service auto-restarts on failure
- deploy.sh bootstraps a fresh Ubuntu server end-to-end
- age encryption workflow protects the wallet private key
</success_criteria>

<output>
After completion, create `.planning/phases/05-hetzner-deployment/05-01-SUMMARY.md`
</output>
