---
phase: 03-websocket-trades
plan: 03
type: execute
---

<objective>
Build the core WebSocket trade listener: event parsing, single-connection lifecycle, producer-consumer queue architecture, and batch DB insertion.

Purpose: Capture every trade from Polymarket's WebSocket stream into TimescaleDB — this is irreplaceable real-time data.
Output: Working TradeListener class with parse_trade_event(), single-connection logic, queue drain, and mock-based tests.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-websocket-trades/03-RESEARCH.md
@.planning/phases/03-websocket-trades/03-CONTEXT.md

# Prior plan outputs:
@.planning/phases/03-websocket-trades/03-01-SUMMARY.md
@.planning/phases/03-websocket-trades/03-02-SUMMARY.md

# Key source files:
@src/db/queries/trades.py
@src/db/queries/markets.py
@src/db/models.py
@src/config.py
@src/collector/market_metadata.py
@tests/collector/conftest.py

# Prior phase context:
@.planning/phases/01-setup-database-layer/01-06-SUMMARY.md
@.planning/phases/02-core-collectors/02-01-SUMMARY.md
@.planning/phases/02-core-collectors/02-03-SUMMARY.md

**Tech stack available:** asyncpg, websockets (upgrade to >=16.0), httpx, pydantic, respx
**Established patterns:**
- Collector class: __init__(pool, client, config), collect_once() -> int, never raises
- insert_trades(pool, trades): COPY protocol, tuples (ts, token_id, side, price, size, trade_id)
- get_active_markets(pool): returns MarketRecord list with clob_token_ids
- asyncio.Queue for producer-consumer decoupling (from RESEARCH.md)
- websockets async iterator for auto-reconnect (from RESEARCH.md)

**Constraining decisions:**
- No trade_id in WebSocket events — tuple[5] = None (from RESEARCH.md pitfall 2)
- App-level "PING" every 10s required (separate from protocol ping/pong) (RESEARCH.md pitfall 1)
- Events arrive as JSON arrays OR single dicts — must handle both (RESEARCH.md pitfall 5)
- String types for price/size/timestamp — explicit float()/int() conversion (RESEARCH.md pitfall 6)
- trade_buffer_size=1000 already in CollectorConfig
- ws_host = "wss://ws-subscriptions-clob.polymarket.com" already in PolymarketConfig
- Geoblocking: all local tests use mocked WebSocket, real testing only from Hetzner
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement parse_trade_event and TradeListener class</name>
  <files>src/collector/trade_listener.py, src/config.py, requirements.txt</files>
  <action>
    1. Update requirements.txt: change `websockets>=12.0` to `websockets>=16.0`

    2. Add to CollectorConfig in src/config.py:
       - ws_ping_interval_sec: int = 10
       - ws_max_instruments_per_conn: int = 500
       - trade_batch_drain_timeout_sec: float = 5.0

    3. Create src/collector/trade_listener.py:

    parse_trade_event(event: dict) -> tuple | None:
       - Filter: if event.get("event_type") != "last_trade_price", return None
       - Extract and convert:
         ts = datetime.fromtimestamp(int(event["timestamp"]) / 1000, tz=timezone.utc)
         token_id = event["asset_id"]
         side = event["side"]
         price = float(event["price"])
         size = float(event["size"])
         trade_id = None  (not in WS events — RESEARCH.md pitfall 2)
       - Return tuple: (ts, token_id, side, price, size, trade_id)
       - Wrap in try/except: return None on any conversion error. Log warning with event data.

    TradeListener class:
       __init__(self, pool: asyncpg.Pool, config: CollectorConfig):
         - self.pool = pool
         - self.config = config
         - self._ws_url = get_config().polymarket.ws_host + "/ws/market"
         - self._queue: asyncio.Queue = asyncio.Queue(maxsize=10000)
         - self._running = False
         - self._tasks: list[asyncio.Task] = []

       async def _subscribe(self, ws, token_ids: list[str]) -> None:
         - Send JSON: {"assets_ids": token_ids, "type": "market"}
         - Log: "Subscribed to N tokens"

       async def _ping_loop(self, ws) -> None:
         - While self._running:
           await asyncio.sleep(self.config.ws_ping_interval_sec)
           await ws.send("PING")
         - Handle CancelledError gracefully (just return)

       async def _receive_loop(self, ws) -> None:
         - async for raw in ws:
           parsed = json.loads(raw)
           events = parsed if isinstance(parsed, list) else [parsed]
           for event in events:
             trade = parse_trade_event(event)
             if trade is not None:
               try:
                 self._queue.put_nowait(trade)
               except asyncio.QueueFull:
                 logger.warning("Trade queue full, dropping event")
         - Use put_nowait to avoid blocking the receive loop. Drop events if queue is full (better than blocking WS and missing heartbeat).

       async def _drain_loop(self) -> None:
         - While self._running or not self._queue.empty():
           batch = []
           try:
             # Wait for first item (with timeout so we can check _running)
             trade = await asyncio.wait_for(self._queue.get(), timeout=self.config.trade_batch_drain_timeout_sec)
             batch.append(trade)
           except asyncio.TimeoutError:
             continue
           except asyncio.CancelledError:
             break
           # Drain remaining items up to batch_size
           while len(batch) < self.config.trade_buffer_size:
             try:
               batch.append(self._queue.get_nowait())
             except asyncio.QueueEmpty:
               break
           # Insert batch
           if batch:
             try:
               await insert_trades(self.pool, batch)
               logger.info("Inserted %d trades", len(batch))
             except Exception:
               logger.error("Failed to insert %d trades", len(batch), exc_info=True)

       async def _listen_single(self, token_ids: list[str]) -> None:
         - Uses websockets async iterator for auto-reconnect:
           from websockets.asyncio.client import connect
           async for ws in connect(self._ws_url):
             try:
               await self._subscribe(ws, token_ids)
               ping_task = asyncio.create_task(self._ping_loop(ws))
               try:
                 await self._receive_loop(ws)
               finally:
                 ping_task.cancel()
                 try:
                   await ping_task
                 except asyncio.CancelledError:
                   pass
             except ConnectionClosed:
               logger.warning("WebSocket disconnected, reconnecting...")
               continue
             except asyncio.CancelledError:
               break

    AVOID:
    - Custom reconnect/retry logic — websockets async iterator handles this (DON'T HAND ROLL from RESEARCH.md)
    - Blocking the event loop with sync calls — everything is async
    - Processing messages synchronously in the receive loop — use queue to decouple
    - await queue.put() in receive loop — use put_nowait to avoid blocking (heartbeat would fail)
  </action>
  <verify>python -c "from src.collector.trade_listener import TradeListener, parse_trade_event; print('import OK')"</verify>
  <done>TradeListener class and parse_trade_event function exist. CollectorConfig has new WS fields. requirements.txt updated.</done>
</task>

<task type="auto">
  <name>Task 2: Write mock-based tests for parsing and listener components</name>
  <files>tests/collector/test_trade_listener.py</files>
  <action>
    Create tests/collector/test_trade_listener.py with mock-based tests.
    Use unittest.mock.AsyncMock for WebSocket object mocking.
    Use @pytest.mark.asyncio for all async tests.

    parse_trade_event tests:

    1. test_parse_valid_trade_event:
       - Input: {"event_type": "last_trade_price", "asset_id": "tok123", "market": "cond1", "price": "0.52", "size": "100", "side": "BUY", "timestamp": "1700000000000"}
       - Assert tuple: (datetime(2023,11,14,22,13,20,tzinfo=utc), "tok123", "BUY", 0.52, 100.0, None)

    2. test_parse_sell_side:
       - Same but side="SELL" -> tuple has "SELL"

    3. test_parse_filters_non_trade_event:
       - Input: {"event_type": "price_change", ...}
       - Returns None

    4. test_parse_invalid_price_returns_none:
       - Input: {"event_type": "last_trade_price", "price": "invalid", ...}
       - Returns None (no exception)

    5. test_parse_missing_fields_returns_none:
       - Input: {"event_type": "last_trade_price"} (missing asset_id, price, etc.)
       - Returns None

    6. test_parse_trade_id_always_none:
       - Verify tuple[5] is always None regardless of input

    _subscribe tests:

    7. test_subscribe_sends_correct_json:
       - Create mock ws (AsyncMock)
       - Call _subscribe(ws, ["tok1", "tok2"])
       - Assert ws.send called with json.dumps({"assets_ids": ["tok1", "tok2"], "type": "market"})

    _ping_loop tests:

    8. test_ping_loop_sends_ping:
       - Create mock ws, create TradeListener with ws_ping_interval_sec=0.1
       - Run _ping_loop as task, let it run ~0.3s, cancel
       - Assert ws.send called with "PING" at least twice

    _drain_loop tests:

    9. test_drain_loop_batches_and_inserts:
       - Create TradeListener with trade_buffer_size=3
       - Pre-fill queue with 5 trade tuples
       - Mock insert_trades
       - Run _drain_loop briefly
       - Assert insert_trades called with batches (first batch size 3, etc.)

    10. test_drain_loop_handles_insert_error:
       - Mock insert_trades to raise Exception
       - Pre-fill queue with trades
       - Run _drain_loop — should log error, not crash

    _receive_loop tests:

    11. test_receive_loop_handles_array_message:
       - Mock ws to yield json.dumps([event1, event2]) then raise CancelledError
       - Run _receive_loop
       - Assert 2 trades in queue

    12. test_receive_loop_handles_dict_message:
       - Mock ws to yield json.dumps(event1) then raise CancelledError
       - Assert 1 trade in queue

    Use conftest.py migrated_pool fixture for tests that need DB (drain_loop with real insert_trades).
    For unit tests (parse, subscribe, ping), no DB needed.
  </action>
  <verify>pytest tests/collector/test_trade_listener.py -v -- all tests pass</verify>
  <done>12 tests covering parse_trade_event edge cases, subscribe format, PING loop, drain batching, and receive loop message handling</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] pytest tests/collector/test_trade_listener.py -v -- all tests pass
- [ ] python -c "from src.collector.trade_listener import TradeListener, parse_trade_event" succeeds
- [ ] No import errors or warnings
- [ ] requirements.txt has websockets>=16.0
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- parse_trade_event handles all RESEARCH.md pitfalls (string types, missing trade_id, arrays vs dicts, event filtering)
- TradeListener has working single-connection logic (subscribe, PING, receive, drain)
- 12 mock-based tests pass
- No real WebSocket connections (all mocked — geoblocking constraint)
</success_criteria>

<output>
After completion, create `.planning/phases/03-websocket-trades/03-03-SUMMARY.md`:

# Phase 03 Plan 03: WebSocket Trade Listener Core Summary

**[Substantive one-liner]**

## Accomplishments
## Files Created/Modified
## Decisions Made
## Issues Encountered
## Next Step

Ready for 03-04-PLAN.md (Connection Pooling + Integration)
</output>
