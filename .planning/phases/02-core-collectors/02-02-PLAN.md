---
phase: 02-core-collectors
plan: 02
type: execute
---

<objective>
Implement the price snapshot collector that paginates Gamma API events and bulk-inserts per-token price tuples every 60 seconds.

Purpose: Captures minute-level price data — the most critical time-series data for ML model training. Every minute without collection is permanently lost training data. This is the highest-volume collector (~8,000 rows per cycle).
Output: src/collector/price_snapshots.py with PriceSnapshotCollector class, tests with respx-mocked Gamma API.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plan summary (collector patterns established):
@.planning/phases/02-core-collectors/02-01-SUMMARY.md

# Source files:
@src/collector/market_metadata.py
@src/utils/client.py
@src/utils/retry.py
@src/scanner/arbitrage.py
@src/config.py
@src/db/queries/prices.py
@src/db/models.py
@tests/collector/conftest.py

**Tech stack available:** asyncpg, respx, httpx, timescaledb, pydantic
**Established patterns:**
- Collector class pattern from 02-01 (pool, client, config constructor; collect_once() returning int)
- respx mocking pattern from 02-01 tests
- Gamma API pagination via client.get_all_active_markets()
- COPY protocol for price bulk inserts (insert_price_snapshots)
- Stringified JSON parsing for clobTokenIds and outcomePrices

**Constraining decisions:**
- [01-05]: COPY protocol via pool.copy_records_to_table() for price inserts (10-100x faster than executemany)
- [01-05]: insert_price_snapshots expects list[tuple] of (ts, token_id, price, volume_24h) — ts must be timezone-aware
- Price snapshots table is per-token (not per-market): each token gets its own row
- Gamma API outcomePrices and clobTokenIds are stringified JSON arrays — must json.loads() before use
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement PriceSnapshotCollector</name>
  <files>src/collector/price_snapshots.py</files>
  <action>
  Create `src/collector/price_snapshots.py` with class `PriceSnapshotCollector`:

  Constructor: `__init__(self, pool: asyncpg.Pool, client: PolymarketClient, config: CollectorConfig)` — same pattern as MarketMetadataCollector.

  Method: `_extract_price_tuples(self, events: list[dict], ts: datetime) -> list[tuple]` — Transforms Gamma API events into price snapshot tuples:
  - Iterate events, then `event.get("markets", [])` for each market
  - For each market:
    - Parse `clobTokenIds` from stringified JSON via json.loads() in try/except (skip market on failure, log warning)
    - Parse `outcomePrices` from stringified JSON via json.loads() in try/except (skip on failure)
    - Get volume_24h from `volume24hr` field: `float(market.get("volume24hr", 0) or 0)`
    - Zip token_ids and prices: for each (token_id, price_str) pair, if token_id is truthy, append (ts, str(token_id), float(price_str), volume_24h)
  - Return list of tuples
  - Use `from datetime import datetime, timezone` for timezone-aware timestamps

  Method: `async collect_once(self) -> int` — One collection cycle:
  - ts = datetime.now(timezone.utc)
  - Call gamma_limiter.acquire() from src.utils.retry
  - Call `await self.client.get_all_active_markets()`
  - Call `self._extract_price_tuples(events, ts)`
  - If no tuples, log info and return 0
  - Call `await insert_price_snapshots(self.pool, tuples)` from src.db.queries.prices
  - Log: `logger.info("Inserted %d price snapshots from %d events", count, len(events))`
  - Return count
  - Wrap in try/except Exception: log error, return 0

  Use `logger = logging.getLogger(__name__)` at module level.
  </action>
  <verify>python -c "from src.collector.price_snapshots import PriceSnapshotCollector; print('import OK')"</verify>
  <done>PriceSnapshotCollector with collect_once() and _extract_price_tuples() implemented.</done>
</task>

<task type="auto">
  <name>Task 2: Write respx-mocked tests for PriceSnapshotCollector</name>
  <files>tests/collector/test_price_snapshots.py</files>
  <action>
  Create `tests/collector/test_price_snapshots.py` with these test cases:

  **Unit tests (no DB needed):**

  a. `test_extract_price_tuples_basic` — Given 1 event with 2 markets, each having 2 tokens (YES/NO):
     - Market 1: clobTokenIds='["tok_a","tok_b"]', outcomePrices='["0.65","0.35"]', volume24hr=5000
     - Market 2: clobTokenIds='["tok_c","tok_d"]', outcomePrices='["0.80","0.20"]', volume24hr=12000
     Verify returns 4 tuples. Each tuple has (ts, token_id, float_price, float_volume). Verify specific values: tok_a→0.65, tok_b→0.35, tok_c→0.80, tok_d→0.20.

  b. `test_extract_price_tuples_malformed_prices` — Market with `outcomePrices: "not valid json"`. Verify that market is skipped (no tuples from it), other markets still processed, no exception.

  c. `test_extract_price_tuples_empty_token_id` — Market where clobTokenIds has empty string entries like '["tok_a",""]'. Verify only tok_a produces a tuple — empty token_id is skipped.

  d. `test_extract_price_tuples_missing_volume` — Market with no `volume24hr` field. Verify volume defaults to 0.0 in tuples.

  **Integration tests (respx + migrated_pool):**

  e. `test_collect_once_success` — Mock Gamma /events returning 2 events with 3 markets total (6 tokens). Call collect_once(). Verify return count = 6. Verify price_snapshots table has 6 rows via `get_price_count(pool)`.

  f. `test_collect_once_bulk_insert_many` — Mock Gamma /events with response containing 20 markets × 2 tokens = 40 tuples. Verify all 40 inserted. Verify `get_latest_prices()` returns correct prices for 2-3 sampled token_ids.

  g. `test_collect_once_api_error` — Mock Gamma /events returning 500. Verify collect_once() returns 0, no exception.

  Use `@respx.mock` decorator and `@pytest.mark.asyncio`. Follow same patterns from 02-01 tests. Sample Gamma event/market data should use realistic field names (conditionId, clobTokenIds as stringified JSON strings, outcomePrices as stringified JSON strings, volume24hr).
  </action>
  <verify>cd C:/Users/nicko/Desktop/poly/polymarket-stat-arb && python -m pytest tests/collector/test_price_snapshots.py -v</verify>
  <done>All price snapshot collector tests pass. Tests cover: tuple extraction with correct types, malformed data handling, empty tokens, missing volume, full collect_once with COPY bulk insert, high-volume insert, and API error resilience.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -m pytest tests/collector/test_price_snapshots.py -v` — all tests pass
- [ ] `python -m pytest tests/collector/ -v` — all collector tests pass (02-01 + 02-02)
- [ ] `python -m pytest tests/db/ -v` — existing DB tests still pass
</verification>

<success_criteria>

- PriceSnapshotCollector implemented with collect_once()
- Price tuples correctly extracted from Gamma API stringified JSON per-token
- COPY protocol bulk insert working via existing insert_price_snapshots()
- All tests pass including integration tests with migrated_pool
- No regressions in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-collectors/02-02-SUMMARY.md`
</output>
