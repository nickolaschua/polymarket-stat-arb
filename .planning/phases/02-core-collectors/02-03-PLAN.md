---
phase: 02-core-collectors
plan: 03
type: execute
---

<objective>
Implement the orderbook snapshot collector that queries active markets from the database, batch-fetches orderbooks from the CLOB API (sync-to-async wrapping), and inserts JSONB orderbook snapshots.

Purpose: Orderbook depth data captures market microstructure — bid/ask spreads, liquidity depth, and order flow patterns needed for execution optimization and market quality assessment.
Output: src/collector/orderbook_snapshots.py with OrderbookSnapshotCollector class, tests with mocked CLOB client.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plan summaries:
@.planning/phases/02-core-collectors/02-01-SUMMARY.md
@.planning/phases/02-core-collectors/02-02-SUMMARY.md

# Source files:
@src/collector/market_metadata.py
@src/collector/price_snapshots.py
@src/utils/client.py
@src/utils/retry.py
@src/utils/heartbeat.py
@src/config.py
@src/db/queries/markets.py
@src/db/queries/orderbooks.py
@src/db/models.py
@tests/collector/conftest.py

**Tech stack available:** asyncpg, respx, httpx, timescaledb, py-clob-client, pydantic
**Established patterns:**
- Collector class pattern from 02-01/02-02 (pool, client, config constructor; collect_once() returning int)
- CLOB sync-to-async wrapping via run_in_executor (pattern from heartbeat.py:147)
- executemany with $x::jsonb cast for orderbook inserts (01-06)
- Per-connection JSONB codec for dict round-trip on reads (01-06)
- clob_read_limiter at 1000/10s
- unittest.mock for CLOB client mocking (sync API, not HTTP)

**Constraining decisions:**
- [01-06]: executemany with $x::jsonb cast — COPY cannot encode Python dicts to JSONB
- [01-06]: insert_orderbook_snapshots expects list[tuple] of (ts, token_id, bids_dict, asks_dict, spread, midpoint)
- py-clob-client is synchronous — MUST use run_in_executor() for all CLOB calls in async context
- CLOB get_order_books() takes list of BookParams, returns list of orderbook dicts
- Orderbook dict structure from CLOB: {"bids": [{"price": "0.55", "size": "100"}, ...], "asks": [...]}
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement OrderbookSnapshotCollector</name>
  <files>src/collector/orderbook_snapshots.py</files>
  <action>
  Create `src/collector/orderbook_snapshots.py` with class `OrderbookSnapshotCollector`:

  Constructor: `__init__(self, pool: asyncpg.Pool, client: PolymarketClient, config: CollectorConfig)` — same pattern as other collectors.

  Method: `_extract_orderbook_tuple(self, token_id: str, book: dict, ts: datetime) -> tuple` — Transforms a single CLOB orderbook response into a DB-ready tuple:
  - book has structure: {"bids": [{"price": "0.55", "size": "100"}, ...], "asks": [...]}
  - Extract bids: list of [float(price), float(size)] pairs from book.get("bids", [])
  - Extract asks: list of [float(price), float(size)] pairs from book.get("asks", [])
  - Store as JSONB-ready dicts: {"levels": [[price, size], ...]} for bids and asks
  - Compute best_bid: bids[0] price if bids exist, else None
  - Compute best_ask: asks[0] price if asks exist, else None
  - spread = best_ask - best_bid if both exist, else None
  - midpoint = (best_ask + best_bid) / 2 if both exist, else None
  - Return tuple: (ts, token_id, bids_dict, asks_dict, spread, midpoint)

  Method: `async _fetch_orderbooks(self, token_ids: list[str]) -> list[dict]` — Async wrapper for sync CLOB:
  - Call `await clob_read_limiter.acquire()` from src.utils.retry
  - Use `loop = asyncio.get_running_loop()`
  - Call `await loop.run_in_executor(None, self.client.get_orderbooks, token_ids)`
  - Return result (list of orderbook dicts)
  - Handle exceptions: log warning, return empty list

  Method: `async collect_once(self) -> int` — One collection cycle:
  - Query active markets from DB: `await get_active_markets(self.pool)` from src.db.queries.markets
  - Extract all token IDs: flatten each market's clob_token_ids list
  - If no tokens, log info ("No active markets found, skipping orderbook collection") and return 0
  - ts = datetime.now(timezone.utc)
  - Chunk token_ids into batches of 20 (configurable, prevents overwhelming CLOB API and avoids excessively large single requests)
  - For each chunk:
    - Call `await self._fetch_orderbooks(chunk)`
    - For each (token_id, book) pair from zip(chunk, books): call _extract_orderbook_tuple()
    - Handle case where CLOB returns fewer books than requested (zip handles this naturally)
  - Collect all tuples into a flat list
  - Call `await insert_orderbook_snapshots(self.pool, tuples)` from src.db.queries.orderbooks
  - Log: `logger.info("Inserted %d orderbook snapshots for %d tokens", count, len(all_token_ids))`
  - Return count
  - Wrap entire body in try/except Exception: log error, return 0

  **CRITICAL: All calls to self.client.get_orderbooks() MUST go through run_in_executor().** The py-clob-client is synchronous. Calling it directly in async context blocks the event loop, preventing other collectors from running in the daemon (Phase 4). Pattern from src/utils/heartbeat.py line 147.

  Use `logger = logging.getLogger(__name__)` at module level.
  </action>
  <verify>python -c "from src.collector.orderbook_snapshots import OrderbookSnapshotCollector; print('import OK')"</verify>
  <done>OrderbookSnapshotCollector with collect_once(), _extract_orderbook_tuple(), _fetch_orderbooks() implemented. All sync CLOB calls wrapped with run_in_executor().</done>
</task>

<task type="auto">
  <name>Task 2: Write mock-based tests for OrderbookSnapshotCollector</name>
  <files>tests/collector/test_orderbook_snapshots.py</files>
  <action>
  Create `tests/collector/test_orderbook_snapshots.py` with these test cases:

  **Unit tests (no DB needed):**

  a. `test_extract_orderbook_tuple_basic` — CLOB book with 3 bids [("0.55","100"),("0.54","200"),("0.53","150")] and 3 asks [("0.56","100"),("0.57","200"),("0.58","300")]. Verify:
     - bids dict: {"levels": [[0.55,100.0],[0.54,200.0],[0.53,150.0]]}
     - asks dict: {"levels": [[0.56,100.0],[0.57,200.0],[0.58,300.0]]}
     - spread: 0.56 - 0.55 = 0.01
     - midpoint: (0.55 + 0.56) / 2 = 0.555

  b. `test_extract_orderbook_tuple_empty_book` — Book with empty bids and asks lists. Verify bids={"levels":[]}, asks={"levels":[]}, spread=None, midpoint=None.

  c. `test_extract_orderbook_tuple_one_sided` — Book with bids but no asks. Verify spread=None, midpoint=None. Bids still extracted correctly.

  **Integration tests (migrated_pool + mocked CLOB):**

  d. `test_collect_once_success` — Pre-insert 2 active markets via upsert_market() with known clob_token_ids (2 tokens each = 4 total). Create an OrderbookSnapshotCollector instance. Patch `_fetch_orderbooks` method (using unittest.mock.AsyncMock) to return realistic orderbook dicts for each batch of token IDs. Call collect_once(). Verify return count = 4. Verify orderbook_snapshots table has 4 rows via a direct SQL count query or get_latest_orderbook() for a known token.

  e. `test_collect_once_no_active_markets` — Don't insert any markets. collect_once() should return 0 with log about no active markets.

  f. `test_collect_once_clob_error` — Patch _fetch_orderbooks to raise an exception. Verify collect_once() returns 0 and does not crash.

  g. `test_chunking_behavior` — Pre-insert markets with 50 total token IDs. Patch _fetch_orderbooks with an AsyncMock. Call collect_once(). Verify _fetch_orderbooks was called ceil(50/20) = 3 times (batches of 20, 20, 10). Verify total orderbook count = 50.

  **Mocking approach:** Since the CLOB API is synchronous and wrapped in run_in_executor, the cleanest mock strategy is to patch `_fetch_orderbooks` on the collector instance (or class) with `unittest.mock.AsyncMock`. This avoids needing to mock the executor. For unit tests of _extract_orderbook_tuple, instantiate the collector with pool=None, client=None (method doesn't use them).

  Use `@pytest.mark.asyncio` for all async tests. Use `unittest.mock.AsyncMock` and `unittest.mock.patch.object` for mocking.
  </action>
  <verify>cd C:/Users/nicko/Desktop/poly/polymarket-stat-arb && python -m pytest tests/collector/test_orderbook_snapshots.py -v</verify>
  <done>All orderbook snapshot collector tests pass. Tests cover: tuple extraction with spread/midpoint, empty/one-sided books, full collect_once with mocked CLOB + real DB, no-markets case, CLOB error resilience, and chunking behavior verified.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -m pytest tests/collector/test_orderbook_snapshots.py -v` — all tests pass
- [ ] `python -m pytest tests/collector/ -v` — all collector tests pass (02-01 + 02-02 + 02-03)
- [ ] `python -m pytest tests/db/ -v` — existing DB tests still pass
- [ ] Phase 2: Core Collectors complete — all 3 collectors implemented and tested
</verification>

<success_criteria>

- OrderbookSnapshotCollector implemented with collect_once()
- Sync CLOB calls properly wrapped with run_in_executor()
- JSONB orderbook data correctly extracted with spread/midpoint computation
- Token chunking prevents overwhelming CLOB API (batches of 20)
- All tests pass including integration with migrated_pool
- No regressions in existing tests
- Phase 2 complete: all 3 core collectors implemented and tested
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-collectors/02-03-SUMMARY.md`

Include: "Phase 2 complete. All 3 core collectors implemented and tested: market metadata (Gamma API → upserts), price snapshots (Gamma API → COPY bulk insert), orderbook snapshots (CLOB → JSONB executemany)."
</output>
