---
phase: 02-core-collectors
plan: 01
type: execute
---

<objective>
Implement the market metadata collector that paginates Gamma API events and upserts market data to the markets table.

Purpose: First collector establishes the collector package structure and test patterns (respx mocking) that all subsequent collectors will follow. Market metadata is the prerequisite for other collectors — it populates the markets table that the orderbook collector queries for active token IDs.
Output: src/collector/market_metadata.py with MarketMetadataCollector class, tests with respx-mocked Gamma API.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Phase 1 summaries (provides DB layer context):
@.planning/phases/01-setup-database-layer/01-01-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-04-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-06-SUMMARY.md

# Source files — existing patterns to follow:
@src/utils/client.py
@src/utils/retry.py
@src/scanner/main.py
@src/scanner/arbitrage.py
@src/config.py
@src/db/queries/markets.py
@src/db/models.py
@tests/conftest.py
@tests/db/conftest.py

**Tech stack available:** asyncpg, testcontainers, timescaledb-pg17, pydantic, httpx, respx (to be installed)
**Established patterns:**
- Pool singleton matching get_config()
- Session-scoped testcontainer with function-scoped pool
- Upsert via INSERT ... ON CONFLICT DO UPDATE
- Gamma API pagination in client.get_all_active_markets()
- Stringified JSON parsing in scanner/arbitrage.py (_parse_stringified_json_array)
- Rate limiting via gamma_limiter (200/10s)

**Constraining decisions:**
- [01-04]: Batch upsert via simple loop (not executemany) — 5-min interval not perf-critical
- [01-04]: upsert_market() expects dict with keys: condition_id, question, slug, market_type, outcomes, clob_token_ids, active, closed, end_date_iso
- Gamma API returns stringified JSON arrays for outcomePrices and clobTokenIds
- py-clob-client is synchronous — all CLOB calls need run_in_executor() (not relevant for this plan, but for 02-03)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create collector package and implement MarketMetadataCollector</name>
  <files>requirements.txt, src/collector/__init__.py, src/collector/market_metadata.py</files>
  <action>
  1. Add `respx>=0.21.0` to requirements.txt (in the Development section) and run `pip install respx`.

  2. Create `src/collector/__init__.py` (empty package init).

  3. Create `src/collector/market_metadata.py` with class `MarketMetadataCollector`:

     Constructor: `__init__(self, pool: asyncpg.Pool, client: PolymarketClient, config: CollectorConfig)` — stores pool, client, config as instance attributes.

     Method: `_extract_market_data(self, raw_market: dict) -> Optional[dict]` — Transforms a single Gamma API market object into the dict format expected by `upsert_market()`:
     - Map `conditionId` to `condition_id` (try both `conditionId` and `condition_id` keys defensively)
     - Return None if condition_id is empty/missing
     - Parse `clobTokenIds` from stringified JSON: `json.loads(raw)` in try/except, fallback to empty list
     - Parse `outcomes` — may be stringified JSON or native list, handle both with isinstance check
     - Map `endDateIso` (or `end_date_iso`) to `end_date_iso`
     - Include `question`, `slug`, `active` (default True), `closed` (default False)
     - Map `marketType` to `market_type` (try both camelCase and snake_case keys)

     Method: `_extract_markets_from_events(self, events: list[dict]) -> list[dict]` — Iterate events, extract nested markets via `event.get("markets", [])`, call `_extract_market_data()` on each, filter out None results. Return flat list of market dicts.

     Method: `async collect_once(self) -> int` — One collection cycle:
     - Call `gamma_limiter.acquire()` from src.utils.retry
     - Call `await self.client.get_all_active_markets()` (already handles pagination with 0.1s inter-page sleep)
     - Call `self._extract_markets_from_events(events)`
     - Call `await upsert_markets(self.pool, market_dicts)` from src.db.queries.markets
     - Log: `logger.info("Upserted %d markets from %d events", len(market_dicts), len(events))`
     - Return len(market_dicts)
     - Wrap entire body in try/except Exception: log error with logger.error(), return 0. Never crash — the daemon will call this repeatedly.

  Use `logger = logging.getLogger(__name__)` at module level.
  </action>
  <verify>python -c "from src.collector.market_metadata import MarketMetadataCollector; print('import OK')"</verify>
  <done>MarketMetadataCollector class exists with collect_once(), _extract_market_data(), _extract_markets_from_events() methods. respx installed in requirements.txt.</done>
</task>

<task type="auto">
  <name>Task 2: Write respx-mocked tests for MarketMetadataCollector</name>
  <files>tests/collector/__init__.py, tests/collector/conftest.py, tests/collector/test_market_metadata.py</files>
  <action>
  1. Create `tests/collector/__init__.py` (empty).

  2. Create `tests/collector/conftest.py` with shared fixtures:
     - `mock_client` fixture: Creates a PolymarketClient with default config. This provides the async HTTP client for Gamma API calls (which respx will intercept).

  3. Create `tests/collector/test_market_metadata.py` with these test cases:

     **Unit tests (no DB, no respx needed):**

     a. `test_extract_market_data_basic` — Create a sample Gamma API market dict with fields:
        `{"conditionId": "0xabc123", "question": "Will X happen?", "slug": "will-x-happen", "clobTokenIds": "[\"token_yes\",\"token_no\"]", "outcomePrices": "[\"0.55\",\"0.45\"]", "outcomes": "[\"Yes\",\"No\"]", "active": true, "closed": false, "endDateIso": "2026-03-01T00:00:00Z", "marketType": "binary"}`
        Verify _extract_market_data() returns dict with condition_id="0xabc123", clob_token_ids=["token_yes","token_no"], outcomes=["Yes","No"], end_date_iso="2026-03-01T00:00:00Z", market_type="binary", etc.

     b. `test_extract_market_data_missing_condition_id` — Dict with no conditionId/condition_id. Returns None.

     c. `test_extract_market_data_native_list_outcomes` — `outcomes` as native Python list ["Yes","No"] (not stringified). Verify it works without json.loads.

     d. `test_extract_markets_from_events` — List of 2 events, each with "markets" list of 2 items. Returns flat list of 4 market dicts. Also test event with empty markets list returns nothing for that event.

     **Integration tests (respx + migrated_pool):**

     e. `test_collect_once_success` — Use `@respx.mock` decorator. Mock `GET https://gamma-api.polymarket.com/events` to return a JSON list of 2 events, each with 1-2 markets. Create MarketMetadataCollector with migrated_pool and mock_client. Call collect_once(). Verify return count matches expected markets. Verify markets exist in DB via get_market() for a known condition_id.

     f. `test_collect_once_pagination` — Mock first call to /events to return exactly 100 events (triggering pagination). Mock second call with offset=100 to return 20 events. Mock third call with offset=200 to return empty []. Verify total markets upserted = sum of markets in all 120 events. Use `respx.get(url).side_effect` with a callable that checks query params to return different responses per page.

     g. `test_collect_once_api_error` — Mock /events to return httpx.Response(500). Verify collect_once() returns 0, does not raise.

  Use `@pytest.mark.asyncio` for all async tests. For integration tests requiring DB, depend on `migrated_pool` fixture from tests/db/conftest.py. For unit tests, instantiate MarketMetadataCollector with pool=None (methods being tested don't use pool).

  **Respx pattern for pagination:**
  ```python
  call_count = 0
  def gamma_side_effect(request):
      nonlocal call_count
      call_count += 1
      offset = int(request.url.params.get("offset", 0))
      if offset == 0:
          return httpx.Response(200, json=page_1_events)
      elif offset == 100:
          return httpx.Response(200, json=page_2_events)
      else:
          return httpx.Response(200, json=[])
  respx.get("https://gamma-api.polymarket.com/events").mock(side_effect=gamma_side_effect)
  ```
  </action>
  <verify>cd C:/Users/nicko/Desktop/poly/polymarket-stat-arb && python -m pytest tests/collector/test_market_metadata.py -v</verify>
  <done>All market metadata collector tests pass. Tests cover: field extraction, stringified JSON parsing, missing data handling, native list handling, full collect_once flow with mocked API + real DB, pagination across multiple pages, and API error resilience.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -m pytest tests/collector/test_market_metadata.py -v` — all tests pass
- [ ] `python -m pytest tests/db/ -v` — existing 46 DB tests still pass (no regressions)
- [ ] `python -c "from src.collector.market_metadata import MarketMetadataCollector"` — import works
</verification>

<success_criteria>

- MarketMetadataCollector class implemented with collect_once()
- respx installed and available for HTTP mocking
- All tests pass including integration tests with migrated_pool + respx
- Existing 46 DB tests still pass
- Collector handles API errors gracefully (log + return 0, never crash)
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-collectors/02-01-SUMMARY.md`
</output>
