---
phase: 01-setup-database-layer
plan: 03
type: execute
---

<objective>
Create all TimescaleDB schema migrations (tables, hypertables, indexes, compression policies, continuous aggregates, retention policies) and Pydantic DB record models.

Purpose: Define the complete data model for the collection pipeline — 5 core tables with TimescaleDB time-series features. Pydantic models provide type-safe Python representations for query results.
Output: SQL migration files 002-008, src/db/models.py, passing schema verification tests.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/phase-prompt.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-setup-database-layer/01-RESEARCH.md
@.planning/phases/01-setup-database-layer/01-01-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-02-SUMMARY.md

@src/db/migrations/runner.py
@src/db/migrations/001_extensions.sql
@tests/conftest.py

**From research — schema design:**

Tables:
- markets: condition_id (PK), question, slug, market_type, outcomes (TEXT[]), clob_token_ids (TEXT[]), active BOOL, closed BOOL, end_date_iso TEXT, created_at TIMESTAMPTZ, updated_at TIMESTAMPTZ
- price_snapshots: ts TIMESTAMPTZ NOT NULL, token_id TEXT NOT NULL, price DOUBLE PRECISION NOT NULL, volume_24h DOUBLE PRECISION → hypertable by_range('ts', INTERVAL '1 day')
- orderbook_snapshots: ts TIMESTAMPTZ NOT NULL, token_id TEXT NOT NULL, bids JSONB, asks JSONB, spread DOUBLE PRECISION, midpoint DOUBLE PRECISION → hypertable by_range('ts', INTERVAL '7 days')
- trades: ts TIMESTAMPTZ NOT NULL, token_id TEXT NOT NULL, side TEXT NOT NULL, price DOUBLE PRECISION NOT NULL, size DOUBLE PRECISION NOT NULL, trade_id TEXT → hypertable by_range('ts', INTERVAL '1 day')
- resolutions: condition_id TEXT PRIMARY KEY, outcome TEXT, winner_token_id TEXT, resolved_at TIMESTAMPTZ, payout_price DOUBLE PRECISION, detection_method TEXT, created_at TIMESTAMPTZ DEFAULT NOW()

**TimescaleDB features:**
- Compression: segmentby = 'token_id', orderby = 'ts DESC' for price_snapshots and orderbook_snapshots. Compress after 7 days.
- Continuous aggregates: price_candles_1h (OHLCV from price_snapshots), trade_volume_1h (from trades)
- Retention: Drop raw price_snapshots older than 90 days, raw trades older than 90 days. Keep aggregates forever.
- Indexes: (token_id, ts DESC) on price_snapshots, orderbook_snapshots, trades. GIN index on orderbook bids/asks if queried.

**Pitfalls to avoid:**
- chunk_time_interval: 1 day for price_snapshots (11.5M rows/day), 7 days for orderbooks and trades
- compress_segmentby on token_id (filter column) — without it, compression ratio is poor and queries slow
- Use `by_range('ts', INTERVAL '...')` syntax (TimescaleDB 2.13+), not old positional syntax
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create core table migrations</name>
  <files>src/db/migrations/002_markets.sql, src/db/migrations/003_price_snapshots.sql, src/db/migrations/004_orderbook_snapshots.sql, src/db/migrations/005_trades.sql, src/db/migrations/006_resolutions.sql</files>
  <action>
Create 5 SQL migration files:

**002_markets.sql:**
- CREATE TABLE markets with: condition_id TEXT PRIMARY KEY, question TEXT NOT NULL, slug TEXT, market_type TEXT, outcomes TEXT[], clob_token_ids TEXT[], active BOOLEAN DEFAULT true, closed BOOLEAN DEFAULT false, end_date_iso TEXT, created_at TIMESTAMPTZ DEFAULT NOW(), updated_at TIMESTAMPTZ DEFAULT NOW()
- CREATE INDEX idx_markets_active ON markets (active) WHERE active = true
- This is NOT a hypertable — it's a regular table for metadata lookups

**003_price_snapshots.sql:**
- CREATE TABLE price_snapshots with: ts TIMESTAMPTZ NOT NULL, token_id TEXT NOT NULL, price DOUBLE PRECISION NOT NULL, volume_24h DOUBLE PRECISION
- SELECT create_hypertable('price_snapshots', by_range('ts', INTERVAL '1 day'))
- CREATE INDEX idx_price_snapshots_token_time ON price_snapshots (token_id, ts DESC)

**004_orderbook_snapshots.sql:**
- CREATE TABLE orderbook_snapshots with: ts TIMESTAMPTZ NOT NULL, token_id TEXT NOT NULL, bids JSONB, asks JSONB, spread DOUBLE PRECISION, midpoint DOUBLE PRECISION
- SELECT create_hypertable('orderbook_snapshots', by_range('ts', INTERVAL '7 days'))
- CREATE INDEX idx_orderbook_snapshots_token_time ON orderbook_snapshots (token_id, ts DESC)

**005_trades.sql:**
- CREATE TABLE trades with: ts TIMESTAMPTZ NOT NULL, token_id TEXT NOT NULL, side TEXT NOT NULL, price DOUBLE PRECISION NOT NULL, size DOUBLE PRECISION NOT NULL, trade_id TEXT
- SELECT create_hypertable('trades', by_range('ts', INTERVAL '1 day'))
- CREATE INDEX idx_trades_token_time ON trades (token_id, ts DESC)
- CREATE UNIQUE INDEX idx_trades_trade_id ON trades (trade_id) WHERE trade_id IS NOT NULL

**006_resolutions.sql:**
- CREATE TABLE resolutions with: condition_id TEXT PRIMARY KEY, outcome TEXT, winner_token_id TEXT, resolved_at TIMESTAMPTZ, payout_price DOUBLE PRECISION, detection_method TEXT, created_at TIMESTAMPTZ DEFAULT NOW()

Do NOT add compression or continuous aggregates here — those go in separate migrations (007, 008) to keep each migration focused and independently reversible.
  </action>
  <verify>pytest tests/db/test_migrations.py -v (existing migration runner test should still pass, and new files should be discovered)</verify>
  <done>5 SQL migration files created. All tables use correct column types. Hypertables created with appropriate chunk intervals. Indexes on (token_id, ts DESC) for time-series tables.</done>
</task>

<task type="auto">
  <name>Task 2: Create TimescaleDB feature migrations</name>
  <files>src/db/migrations/007_continuous_aggs.sql, src/db/migrations/008_compression.sql</files>
  <action>
**007_continuous_aggs.sql:**
Create two continuous aggregates:

1. price_candles_1h — 1-hour OHLCV from price_snapshots:
```sql
CREATE MATERIALIZED VIEW price_candles_1h
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', ts) AS bucket,
    token_id,
    first(price, ts) AS open,
    max(price) AS high,
    min(price) AS low,
    last(price, ts) AS close,
    avg(volume_24h) AS avg_volume
FROM price_snapshots
GROUP BY bucket, token_id;
```
Add refresh policy: start_offset => INTERVAL '3 hours', end_offset => INTERVAL '1 hour', schedule_interval => INTERVAL '1 hour'.

2. trade_volume_1h — 1-hour trade aggregates:
```sql
CREATE MATERIALIZED VIEW trade_volume_1h
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 hour', ts) AS bucket,
    token_id,
    count(*) AS trade_count,
    sum(size) AS total_size,
    sum(price * size) AS total_value,
    avg(price) AS avg_price
FROM trades
GROUP BY bucket, token_id;
```
Add refresh policy: start_offset => INTERVAL '3 hours', end_offset => INTERVAL '1 hour', schedule_interval => INTERVAL '1 hour'.

**008_compression.sql:**
Enable compression on time-series tables:

1. price_snapshots: compress_segmentby = 'token_id', compress_orderby = 'ts DESC'. Compression policy after 7 days.
2. orderbook_snapshots: compress_segmentby = 'token_id', compress_orderby = 'ts DESC'. Compression policy after 7 days.
3. trades: compress_segmentby = 'token_id', compress_orderby = 'ts DESC'. Compression policy after 7 days.

Add retention policies:
- price_snapshots: drop_after => INTERVAL '90 days'
- trades: drop_after => INTERVAL '90 days'
- Do NOT add retention to orderbook_snapshots (less volume, keep longer)
- Do NOT add retention to continuous aggregates (keep forever)

Do NOT enable compression on tables that have continuous aggregates referencing them without checking TimescaleDB compatibility — compression on source tables for continuous aggregates is supported in TimescaleDB 2.x but the ordering matters (aggregate must exist first, which it does since 007 runs before 008).
  </action>
  <verify>Write a quick test or manual check that verifies continuous aggregates and compression policies exist after migration</verify>
  <done>Continuous aggregates price_candles_1h and trade_volume_1h created with refresh policies. Compression enabled on all 3 time-series tables with segmentby=token_id. Retention policies on price_snapshots and trades (90 days).</done>
</task>

<task type="auto">
  <name>Task 3: Create Pydantic DB models and schema verification test</name>
  <files>src/db/models.py, tests/db/test_schema.py</files>
  <action>
**src/db/models.py:**
Create Pydantic models for DB records (used as return types from query functions):

```python
from datetime import datetime
from typing import Optional
from pydantic import BaseModel

class MarketRecord(BaseModel):
    condition_id: str
    question: str
    slug: Optional[str] = None
    market_type: Optional[str] = None
    outcomes: list[str] = []
    clob_token_ids: list[str] = []
    active: bool = True
    closed: bool = False
    end_date_iso: Optional[str] = None
    created_at: datetime
    updated_at: datetime

class PriceSnapshot(BaseModel):
    ts: datetime
    token_id: str
    price: float
    volume_24h: Optional[float] = None

class OrderbookSnapshot(BaseModel):
    ts: datetime
    token_id: str
    bids: Optional[dict] = None  # JSONB
    asks: Optional[dict] = None  # JSONB
    spread: Optional[float] = None
    midpoint: Optional[float] = None

class TradeRecord(BaseModel):
    ts: datetime
    token_id: str
    side: str
    price: float
    size: float
    trade_id: Optional[str] = None

class ResolutionRecord(BaseModel):
    condition_id: str
    outcome: Optional[str] = None
    winner_token_id: Optional[str] = None
    resolved_at: Optional[datetime] = None
    payout_price: Optional[float] = None
    detection_method: Optional[str] = None
    created_at: datetime
```

Add a helper to convert asyncpg Record to Pydantic model:
```python
def record_to_model(record: asyncpg.Record, model_cls: type[BaseModel]) -> BaseModel:
    return model_cls(**dict(record))
```

**tests/db/test_schema.py:**
Integration test that verifies the full schema after running all migrations:
1. Run all migrations via run_migrations()
2. Verify all 5 tables exist (query pg_tables)
3. Verify 3 hypertables exist (query timescaledb_information.hypertables)
4. Verify 2 continuous aggregates exist (query timescaledb_information.continuous_aggregates)
5. Verify compression is enabled on 3 tables (query timescaledb_information.compression_settings)
6. Verify indexes exist on time-series tables
7. Verify retention policies exist (query timescaledb_information.jobs for retention)

Use the db_pool fixture from conftest.py. Run migrations as part of test setup.

Do NOT test individual migration content — just verify the end-state schema is correct. Migration ordering/application is tested in test_migrations.py.
  </action>
  <verify>pytest tests/db/test_schema.py -v</verify>
  <done>Pydantic models for all 5 tables + record_to_model helper. Schema verification test confirms all tables, hypertables, aggregates, compression, and retention policies.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] All 7 migration files exist (001-008) in src/db/migrations/
- [ ] `pytest tests/db/test_schema.py -v` passes — full schema verified
- [ ] `pytest tests/db/test_migrations.py -v` still passes
- [ ] `python -c "from src.db.models import MarketRecord, PriceSnapshot, OrderbookSnapshot, TradeRecord, ResolutionRecord"` imports ok
- [ ] No import errors across any test files
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- 7 SQL migration files cover: extension, 5 tables (3 hypertables), 2 continuous aggregates, compression + retention
- Pydantic models match DB columns for all 5 tables
- Schema verification test confirms TimescaleDB features are active
- No errors or warnings introduced
</success_criteria>

<output>
After completion, create `.planning/phases/01-setup-database-layer/01-03-SUMMARY.md`
</output>
