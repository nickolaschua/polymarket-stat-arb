---
phase: 01-setup-database-layer
plan: 06
type: tdd
---

<objective>
Build query functions for orderbook snapshots (JSONB storage) and trade events (high-volume inserts), then verify the complete database layer works end-to-end.

Purpose: Complete the query function layer for all 5 tables. Orderbooks use JSONB for flexible bid/ask storage. Trades are high-volume from WebSocket feed. Final integration test validates the full pipeline: pool → migrations → schema → models → queries.
Output: src/db/queries/orderbooks.py, src/db/queries/trades.py, passing integration tests, phase complete.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-setup-database-layer/01-RESEARCH.md
@.planning/phases/01-setup-database-layer/01-01-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-02-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-03-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-04-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-05-SUMMARY.md

@src/db/pool.py
@src/db/models.py
@src/db/queries/markets.py
@src/db/queries/prices.py
@src/db/migrations/004_orderbook_snapshots.sql
@src/db/migrations/005_trades.sql
@tests/conftest.py

**From research:**
- Orderbook bids/asks stored as JSONB — asyncpg handles Python dict ↔ JSONB encoding natively
- Trades use standard INSERT (not COPY) — they arrive one-at-a-time from WebSocket, batched in application buffer
- Use `pool.copy_records_to_table()` for trade batch inserts (trade_buffer_size=1000 from CollectorConfig)
- JSONB fields: asyncpg encodes Python dicts as JSONB automatically, decodes JSONB to dict

**Pitfall:** JSONB storage for orderbooks — don't store as TEXT and parse manually. asyncpg handles JSONB natively. Pass Python dicts directly.
</context>

<feature>
  <name>Orderbook + Trade Query Functions</name>
  <files>src/db/queries/orderbooks.py, src/db/queries/trades.py, tests/db/test_orderbooks.py, tests/db/test_trades.py, tests/db/test_integration.py</files>
  <behavior>
    **Orderbook queries:**
    - insert_orderbook_snapshots(pool, snapshots: list[tuple]) → int
      Uses COPY protocol. Each tuple: (ts, token_id, bids: dict, asks: dict, spread: float, midpoint: float)
    - get_latest_orderbook(pool, token_id: str) → OrderbookSnapshot | None
      Returns most recent orderbook for a token.
    - get_orderbook_history(pool, token_id: str, start: datetime, end: datetime, limit: int = 100) → list[OrderbookSnapshot]

    Test cases (orderbooks):
    1. insert_orderbook_snapshots with JSONB dicts → inserted, queryable
    2. get_latest_orderbook → returns most recent snapshot with correct bids/asks dicts
    3. get_orderbook_history with time range → correct filtering
    4. JSONB round-trip: inserted dict == queried dict (no data loss)
    5. Empty bids/asks (None) → stored and returned as None

    **Trade queries:**
    - insert_trades(pool, trades: list[tuple]) → int
      Uses COPY protocol for batch inserts. Each tuple: (ts, token_id, side, price, size, trade_id)
    - get_recent_trades(pool, token_id: str, limit: int = 100) → list[TradeRecord]
      Returns most recent trades for a token, ordered by ts DESC.
    - get_trade_count(pool, token_id: str | None = None) → int
      Count trades, optionally filtered by token.

    Test cases (trades):
    1. insert_trades with 5 records → all inserted
    2. insert_trades with 500 records → bulk insert works
    3. get_recent_trades with limit → respects limit, ordered by ts DESC
    4. get_trade_count with no filter → total count
    5. get_trade_count with token_id filter → filtered count
    6. Duplicate trade_id handling: unique index on trade_id WHERE NOT NULL

    **Integration test (test_integration.py):**
    1. Run all migrations
    2. Upsert 3 markets
    3. Insert 100 price snapshots across 3 token_ids
    4. Insert 10 orderbook snapshots
    5. Insert 50 trades
    6. Upsert 1 resolution
    7. Query each table and verify data integrity
    8. Verify get_unresolved_markets returns correct markets
    9. Verify get_latest_prices returns one per token
    This proves the full database layer works end-to-end.
  </behavior>
  <implementation>
    **orderbooks.py:**
    - insert_orderbook_snapshots: `pool.copy_records_to_table('orderbook_snapshots', records=snapshots, columns=['ts', 'token_id', 'bids', 'asks', 'spread', 'midpoint'])`. asyncpg handles dict→JSONB encoding automatically. Handle empty list.
    - get_latest_orderbook: `SELECT * FROM orderbook_snapshots WHERE token_id = $1 ORDER BY ts DESC LIMIT 1`. Return OrderbookSnapshot or None.
    - get_orderbook_history: `SELECT * FROM orderbook_snapshots WHERE token_id = $1 AND ts >= $2 AND ts <= $3 ORDER BY ts DESC LIMIT $4`.

    **trades.py:**
    - insert_trades: `pool.copy_records_to_table('trades', records=trades, columns=['ts', 'token_id', 'side', 'price', 'size', 'trade_id'])`. Handle empty list. For duplicate trade_id, use a try/except around COPY and fall back to INSERT ... ON CONFLICT DO NOTHING if COPY fails due to unique constraint.
    - get_recent_trades: `SELECT * FROM trades WHERE token_id = $1 ORDER BY ts DESC LIMIT $2`.
    - get_trade_count: `SELECT count(*) FROM trades` (no filter) or `SELECT count(*) FROM trades WHERE token_id = $1` (filtered).

    **test_integration.py:**
    Full end-to-end test using all query modules together. Import from all queries/* modules. Use migrated_pool fixture. This is the "Phase 1 complete" smoke test.

    **JSONB handling note:** asyncpg requires `await conn.set_type_codec('jsonb', encoder=json.dumps, decoder=json.loads, schema='pg_catalog')` for JSONB support on connections created without it. Check if pool-level COPY handles this automatically — if not, set codec in pool's `init` callback or in conftest.py pool setup.

    Do NOT store orderbook bids/asks as TEXT — use native JSONB. asyncpg may need explicit codec setup for COPY with JSONB columns. If COPY doesn't support JSONB natively, fall back to `pool.executemany()` for orderbook inserts only (orderbook volume is much lower than prices, so performance is not critical).
  </implementation>
</feature>

<verification>
pytest tests/db/ -v --tb=short
</verification>

<success_criteria>
- Failing tests written and committed (RED)
- All query functions pass tests (GREEN)
- Refactor if needed (REFACTOR)
- JSONB round-trip works (dict in → dict out)
- Bulk trade inserts handle 500+ records
- Full integration test passes (all 5 tables used together)
- `pytest tests/db/ -v` — ALL database tests pass (migrations, schema, markets, resolutions, prices, orderbooks, trades, integration)
- Phase 1 complete: pool + migrations + schema + models + queries all working together
- All 2-3 commits present
</success_criteria>

<output>
After completion, create `.planning/phases/01-setup-database-layer/01-06-SUMMARY.md` with:
- RED: What tests were written, why they failed
- GREEN: What implementation made them pass
- REFACTOR: What cleanup was done (if any)
- Commits: List of commits produced
- Phase 1 complete status

This is the final plan for Phase 1. Success criteria includes "Phase 1: Setup + Database Layer complete."
</output>
