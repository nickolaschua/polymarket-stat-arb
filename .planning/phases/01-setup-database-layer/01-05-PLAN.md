---
phase: 01-setup-database-layer
plan: 05
type: tdd
---

<objective>
Build query functions for price snapshots using asyncpg's binary COPY protocol for bulk inserts — the critical performance path for the data collection daemon.

Purpose: Price snapshots are the highest-volume table (~8,000 rows every 60 seconds = ~11.5M rows/day). COPY protocol is 10-100x faster than executemany. TDD ensures bulk insert correctness and query behavior against real TimescaleDB hypertables.
Output: src/db/queries/prices.py, passing integration tests.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-setup-database-layer/01-RESEARCH.md
@.planning/phases/01-setup-database-layer/01-01-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-02-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-03-SUMMARY.md
@.planning/phases/01-setup-database-layer/01-04-SUMMARY.md

@src/db/pool.py
@src/db/models.py
@src/db/migrations/003_price_snapshots.sql
@src/db/migrations/007_continuous_aggs.sql
@tests/conftest.py

**From research — bulk COPY pattern:**
```python
async def insert_price_snapshots(pool, snapshots: list[tuple]):
    await pool.copy_records_to_table(
        'price_snapshots',
        records=snapshots,
        columns=['ts', 'token_id', 'price', 'volume_24h'],
    )
```

**Key constraints:**
- Use `pool.copy_records_to_table()` — NOT `pool.executemany()` (10-100x slower)
- Records must be list of tuples, not dicts or Pydantic models
- Column order in `columns` parameter must match tuple field order
- Tuples must use Python datetime objects for TIMESTAMPTZ columns (asyncpg handles encoding)
- Use `ANY($1::text[])` for array parameters in queries, not IN clause

**Pitfall:** Don't cache prepared statements across pool.release() — use pool-level methods (pool.fetch, pool.execute) which handle this internally.
</context>

<feature>
  <name>Price Snapshot Query Functions</name>
  <files>src/db/queries/prices.py, tests/db/test_prices.py</files>
  <behavior>
    **Write functions:**
    - insert_price_snapshots(pool, snapshots: list[tuple]) → int (count inserted)
      Uses COPY protocol. Each tuple: (ts: datetime, token_id: str, price: float, volume_24h: float | None)

    **Read functions:**
    - get_latest_prices(pool, token_ids: list[str]) → list[PriceSnapshot]
      Returns most recent price for each token_id. Uses DISTINCT ON (token_id) ORDER BY ts DESC.
    - get_price_history(pool, token_id: str, start: datetime, end: datetime, limit: int = 1000) → list[PriceSnapshot]
      Returns price snapshots for a single token in time range, ordered by ts DESC.
    - get_price_count(pool) → int
      Returns total row count (for monitoring/health checks).

    Test cases:
    1. insert_price_snapshots with 10 records → get_price_count returns 10
    2. insert_price_snapshots with 1000 records → all inserted (bulk performance)
    3. get_latest_prices for 3 token_ids with multiple snapshots each → returns only the most recent per token
    4. get_latest_prices with non-existent token_ids → returns empty list
    5. get_price_history with time range → returns only snapshots within range
    6. get_price_history respects limit parameter
    7. insert_price_snapshots with empty list → no error, returns 0
    8. Verify data lands in hypertable (query timescaledb_information.hypertables for price_snapshots)
  </behavior>
  <implementation>
    **prices.py:**
    - insert_price_snapshots: `pool.copy_records_to_table('price_snapshots', records=snapshots, columns=['ts', 'token_id', 'price', 'volume_24h'])`. Return len(snapshots). Handle empty list case (return 0 without calling COPY).
    - get_latest_prices: `SELECT DISTINCT ON (token_id) ts, token_id, price, volume_24h FROM price_snapshots WHERE token_id = ANY($1::text[]) ORDER BY token_id, ts DESC`. Convert records to PriceSnapshot models.
    - get_price_history: `SELECT ts, token_id, price, volume_24h FROM price_snapshots WHERE token_id = $1 AND ts >= $2 AND ts <= $3 ORDER BY ts DESC LIMIT $4`. Convert to PriceSnapshot models.
    - get_price_count: `SELECT count(*) FROM price_snapshots`. Return int.

    **Test helpers:**
    - Factory function `make_price_tuple(token_id, price, ts=None)` that creates properly-typed tuples with datetime objects.
    - Use `datetime.now(timezone.utc)` for timestamps (TIMESTAMPTZ requires timezone-aware datetimes with asyncpg).

    **Test fixtures:** Use migrated_pool fixture (runs all migrations). Use clean_db fixture for isolation.

    Do NOT use `pool.executemany()` for inserts — the entire point of this plan is COPY performance.
    Do NOT forget timezone on datetime objects — asyncpg requires timezone-aware datetimes for TIMESTAMPTZ.
  </implementation>
</feature>

<verification>
pytest tests/db/test_prices.py -v
</verification>

<success_criteria>
- Failing tests written and committed (RED)
- COPY-based bulk insert works correctly (GREEN)
- All query functions return correct PriceSnapshot models
- Bulk insert handles 1000+ records without error
- Empty list insert is safe
- Array parameter queries use ANY($1::text[])
- All 2-3 commits present
</success_criteria>

<output>
After completion, create `.planning/phases/01-setup-database-layer/01-05-SUMMARY.md` with:
- RED: What tests were written, why they failed
- GREEN: What implementation made them pass
- REFACTOR: What cleanup was done (if any)
- Commits: List of commits produced
</output>
