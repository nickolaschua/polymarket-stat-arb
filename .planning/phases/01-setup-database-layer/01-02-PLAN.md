---
phase: 01-setup-database-layer
plan: 02
type: tdd
---

<objective>
Build a custom SQL migration runner that applies numbered .sql files in order and tracks applied migrations in a `schema_migrations` table.

Purpose: Provide reliable, ordered schema evolution for TimescaleDB without external dependency (pogo-migrate too immature at v0.0.22). TDD ensures the runner handles edge cases: idempotency, ordering, partial failures.
Output: src/db/migrations/runner.py, src/db/migrations/001_extensions.sql, passing tests.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-setup-database-layer/01-RESEARCH.md
@.planning/phases/01-setup-database-layer/01-01-SUMMARY.md

@src/db/pool.py
@tests/conftest.py

**From research — migration runner pattern:**
```python
async def run_migrations(pool: asyncpg.Pool, migrations_dir: Path):
    # Create schema_migrations table if not exists
    # Get set of already-applied versions
    # Sort .sql files by numeric prefix
    # Apply each unapplied migration in a transaction
    # Record version + filename + timestamp
```

**Key constraints:**
- ~50 lines of code, no external dependency
- Parse version from filename: `int(filename.split("_")[0])` — files named like `001_extensions.sql`
- Each migration applied in its own transaction (atomic per-file)
- schema_migrations table: version INT PRIMARY KEY, filename TEXT NOT NULL, applied_at TIMESTAMPTZ DEFAULT NOW()
- Idempotent: running twice with same files produces no error and no duplicate entries

**Pitfall from research:** TimescaleDB extension must be enabled before any hypertable DDL. First migration (001) must be `CREATE EXTENSION IF NOT EXISTS timescaledb;`.
</context>

<feature>
  <name>SQL Migration Runner</name>
  <files>src/db/migrations/__init__.py, src/db/migrations/runner.py, src/db/migrations/001_extensions.sql, tests/db/test_migrations.py</files>
  <behavior>
    run_migrations(pool, migrations_dir) applies pending SQL files:
    - Creates schema_migrations table if not exists
    - Skips already-applied migrations (idempotent)
    - Applies new migrations in numeric order (001 before 002)
    - Records each applied migration with version, filename, applied_at
    - Wraps each migration in a transaction (rollback on failure)
    - Returns list of newly applied migration filenames

    Test cases:
    1. Empty database → creates schema_migrations table + applies 001_extensions.sql → returns ["001_extensions.sql"]
    2. Run again with same files → returns [] (no new migrations)
    3. Add 002_test.sql → only applies 002, not 001 → returns ["002_test.sql"]
    4. Verify schema_migrations has correct version/filename for all applied
    5. Verify TimescaleDB extension is active after 001 runs
    6. Migration with syntax error → transaction rolls back, migration NOT recorded, raises exception
  </behavior>
  <implementation>
    Create src/db/migrations/ package with __init__.py.
    Create 001_extensions.sql: `CREATE EXTENSION IF NOT EXISTS timescaledb;`

    Implement runner.py with:
    - `async def run_migrations(pool: asyncpg.Pool, migrations_dir: Path) -> list[str]`
    - Use `pool.acquire()` for a single connection throughout (consistent view)
    - CREATE TABLE IF NOT EXISTS schema_migrations (version INT PRIMARY KEY, filename TEXT NOT NULL, applied_at TIMESTAMPTZ DEFAULT NOW())
    - SELECT applied versions into a set
    - sorted(migrations_dir.glob("*.sql")) for file ordering
    - Parse version: int(sql_file.stem.split("_")[0]) — use stem not name to avoid .sql extension issues
    - For each unapplied: `async with conn.transaction(): await conn.execute(sql_text)`
    - Then INSERT into schema_migrations
    - Return list of applied filenames

    Do NOT use pool-level methods for migrations (need single connection for consistent schema_migrations reads). Use `async with pool.acquire() as conn:` throughout.
    Do NOT put the INSERT into schema_migrations inside the same transaction as the DDL — DDL in PostgreSQL auto-commits in some cases, and mixing with DML in the same transaction can cause issues with extension creation. Instead: run DDL in transaction, then INSERT tracking row separately.
  </implementation>
</feature>

<verification>
pytest tests/db/test_migrations.py -v
</verification>

<success_criteria>
- Failing tests written and committed (RED)
- Migration runner implementation passes all tests (GREEN)
- Refactor if needed (REFACTOR)
- 001_extensions.sql enables TimescaleDB
- Runner is idempotent (safe to run multiple times)
- All 2-3 commits present
</success_criteria>

<output>
After completion, create `.planning/phases/01-setup-database-layer/01-02-SUMMARY.md` with:
- RED: What tests were written, why they failed
- GREEN: What implementation made them pass
- REFACTOR: What cleanup was done (if any)
- Commits: List of commits produced
</output>
