---
phase: 04-daemon-supervisor-cli
plan: 02
type: execute
---

<objective>
Add periodic health status logging to the daemon and create the `collect` CLI command to start the data collection pipeline.

Purpose: Health logging provides unattended visibility into daemon operation. The CLI command is the user-facing entry point that wires up pool, migrations, client, and daemon.
Output: Health logging in daemon.py, `collect` command in src/main.py.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-daemon-supervisor-cli/04-01-SUMMARY.md

# Key files:
@src/collector/daemon.py
@src/main.py
@src/config.py
@src/db/pool.py
@src/db/migrations/runner.py
@src/collector/trade_listener.py

**Tech stack available:** asyncpg, click, pydantic
**Established patterns:** Click CLI group in src/main.py, get_pool()/close_pool() singleton, run_migrations(), asyncio.run() in CLI commands

**Constraining decisions:**
- [03-04]: TradeListener.get_health() returns TradeListenerHealth snapshot
- [01-01]: Pool singleton via get_pool()/close_pool()
- [01-02]: Migrations via run_migrations(pool, migrations_dir)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add periodic health logging</name>
  <files>src/collector/daemon.py</files>
  <action>
Add health logging to CollectorDaemon:

**Track per-collector stats** in _run_polling_loop:
- Add `self._collector_stats: dict[str, dict] = {}` in __init__ with keys for each polling collector
- Each entry: `{"last_collect_ts": None, "total_items": 0, "error_count": 0, "last_error": None}`
- In _run_polling_loop after successful collect_once(): update last_collect_ts (datetime.now(UTC)), increment total_items by count
- In _run_polling_loop except block: increment error_count, set last_error to str(exc)

**`_health_log_loop(self)`** — async method, runs every 60 seconds:
- `while self._running:`
  - `await asyncio.sleep(60)`
  - Calculate uptime from self._started_at (add to __init__ and set in run())
  - Count alive/dead tasks: `alive = sum(1 for t in self._tasks.values() if not t.done())`, `dead = sum(1 for t in self._tasks.values() if t.done())`
  - Total restarts: `sum(self._restart_counts.values())`
  - Log header: `logger.info("=== Daemon Health === uptime: %s | tasks: %d alive, %d dead | restarts: %d", uptime_str, alive, dead, total_restarts)`
  - For each polling collector in self._collector_stats:
    - `logger.info("  %s: items=%d errors=%d last=%s", name, stats["total_items"], stats["error_count"], stats["last_collect_ts"])`
  - For TradeListener: `health = self._trade_listener.get_health()`
    - `logger.info("  trades: received=%d inserted=%d conns=%d queue=%d", health.trades_received, health.trades_inserted, health.connections_active, health.queue_depth)`
  - On CancelledError: return

**`get_health(self) -> dict`** — public method:
- Returns dict summarizing all collector states:
  - `"uptime_seconds"`: float since started_at
  - `"tasks_alive"`: int
  - `"tasks_dead"`: int
  - `"total_restarts"`: int
  - `"collectors"`: dict of per-collector stats (copy of _collector_stats)
  - `"trade_listener"`: TradeListenerHealth snapshot (from get_health())
- Use for programmatic health checks (future monitoring)

**Start health log in run()**:
- Add after starting collector tasks: `self._tasks["_health"] = asyncio.create_task(self._health_log_loop())`
- Do NOT restart the health logger on crash — it's non-critical

**Format uptime** as human-readable: `Xh Ym` or `Ym Zs` for short uptimes.

Use logger.info() for all health output — NOT print/Rich. The daemon runs unattended (production on Hetzner via systemd).
  </action>
  <verify>python -c "from src.collector.daemon import CollectorDaemon; print('health logging ok')"</verify>
  <done>Health logging runs every 60s with per-collector stats. get_health() returns summary dict. Stats tracked in _run_polling_loop.</done>
</task>

<task type="auto">
  <name>Task 2: Create `collect` CLI command</name>
  <files>src/main.py</files>
  <action>
Add `collect` command to the existing Click CLI group in src/main.py:

```
@cli.command()
@click.pass_context
def collect(ctx):
    """Start the data collection daemon (runs all collectors 24/7)."""

    async def run_daemon():
        from pathlib import Path
        from src.db.pool import get_pool, close_pool
        from src.db.migrations.runner import run_migrations
        from src.collector.daemon import CollectorDaemon

        config = get_config()

        logger.info("Starting collector daemon...")

        # Initialize database
        pool = await get_pool()
        migrations_dir = Path(__file__).resolve().parent / "db" / "migrations"
        applied = await run_migrations(pool, migrations_dir)
        if applied:
            logger.info("Applied %d migrations", len(applied))

        # Create client and daemon
        client = PolymarketClient(config)
        daemon = CollectorDaemon(pool, client, config.collector)

        try:
            await daemon.run()
        finally:
            await close_pool()
            logger.info("Collector daemon shut down")

    asyncio.run(run_daemon())
```

Key details:
- Import CollectorDaemon, get_pool, close_pool, run_migrations inside the function (lazy imports to avoid circular dependencies and keep CLI startup fast)
- migrations_dir path: `Path(__file__).resolve().parent / "db" / "migrations"` (src/main.py → src/db/migrations/)
- PolymarketClient already imported at top of src/main.py
- Use `get_config()` already imported at top of src/main.py
- The `finally` block ensures pool is closed even on crash
- Do NOT add --once or --dry-run flags — daemon always runs continuously
- Do NOT add WindowsSelectorEventLoopPolicy — asyncpg handles this
- Do NOT catch KeyboardInterrupt — the daemon's signal handler manages SIGINT
  </action>
  <verify>python -m src.main collect --help</verify>
  <done>`python -m src.main collect --help` shows "Start the data collection daemon" help text. Command exists in CLI group.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.collector.daemon import CollectorDaemon; d = CollectorDaemon.__new__(CollectorDaemon); d.get_health"` — get_health method exists
- [ ] `python -m src.main collect --help` — shows help text
- [ ] Health logging uses logger.info (not print/Rich)
- [ ] No circular import errors
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Health logging every 60s with per-collector breakdown
- get_health() returns structured dict
- `collect` CLI command wired up with pool, migrations, client, daemon
- No import errors or circular dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/04-daemon-supervisor-cli/04-02-SUMMARY.md`
</output>
