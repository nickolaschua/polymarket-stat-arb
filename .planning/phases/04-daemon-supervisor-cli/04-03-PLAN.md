---
phase: 04-daemon-supervisor-cli
plan: 03
type: execute
---

<objective>
Write comprehensive unit tests for the CollectorDaemon covering lifecycle management, crash recovery, and health logging.

Purpose: Verify daemon orchestration logic works correctly without requiring real databases or API connections.
Output: `tests/collector/test_daemon.py` with full test coverage of daemon behavior.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-daemon-supervisor-cli/04-01-SUMMARY.md
@.planning/phases/04-daemon-supervisor-cli/04-02-SUMMARY.md

# Key files:
@src/collector/daemon.py
@tests/collector/conftest.py
@tests/collector/test_trade_listener.py

**Tech stack available:** pytest, pytest-asyncio, unittest.mock
**Established patterns:** AsyncMock for async methods, @pytest.mark.asyncio for async tests, mock.patch for dependency injection

**Constraining decisions:**
- [02-01]: Collector pattern: __init__(pool, client, config), collect_once() -> int, never raises
- [03-04]: TradeListener has run()/stop() lifecycle with get_health()
- [STATE]: Pure unit tests — no testcontainers, no real DB, no network access
</context>

<tasks>

<task type="auto">
  <name>Task 1: Unit tests for daemon lifecycle</name>
  <files>tests/collector/test_daemon.py</files>
  <action>
Create tests/collector/test_daemon.py with lifecycle tests:

**Fixtures:**
- `mock_pool`: `AsyncMock(spec=asyncpg.Pool)` — never actually used by mocked collectors
- `mock_client`: `MagicMock()` — PolymarketClient mock
- `mock_config`: Create a real `CollectorConfig()` with short intervals for testing (all intervals = 1 second)
- `daemon`: `CollectorDaemon(mock_pool, mock_client, mock_config)` with all collector classes patched

**Patch all 5 collector constructors** using `@mock.patch("src.collector.daemon.MarketMetadataCollector")` etc. The patches should return AsyncMock instances where `collect_once` returns 5 and TradeListener's `run` is an async no-op, `stop` is an async no-op, `get_health` returns a mock TradeListenerHealth.

**Tests:**

`test_init_creates_all_collectors`:
- Verify all 5 collector classes were instantiated with correct arguments
- MarketMetadataCollector(pool, client, config)
- PriceSnapshotCollector(pool, client, config)
- OrderbookSnapshotCollector(pool, client, config)
- ResolutionTracker(pool, config) — NO client
- TradeListener(pool, config) — NO client

`test_run_starts_all_tasks`:
- Patch asyncio.create_task to track calls
- Call daemon.run() but set _shutdown_event immediately (so it doesn't block)
- Verify create_task was called for: metadata, prices, orderbooks, resolutions, trades, _monitor, _health (7 tasks)

`test_stop_cancels_polling_tasks`:
- Start daemon tasks manually (create mock tasks in self._tasks)
- Call daemon.stop()
- Verify polling task mocks had .cancel() called
- Verify TradeListener.stop() was awaited (not just cancelled)

`test_stop_is_idempotent`:
- Call stop() twice — no error on second call

`test_polling_loop_calls_collect_once`:
- Create a mock collector with collect_once returning 10
- Run _run_polling_loop for ~2 iterations (set _running=False after short delay)
- Verify collect_once was called at least once

`test_polling_loop_survives_exception`:
- Create a mock collector where collect_once raises RuntimeError
- Run _run_polling_loop for ~2 iterations
- Verify loop continued (collect_once called multiple times despite error)
- Verify error was logged

Use `asyncio.wait_for(coroutine, timeout=5.0)` in tests to prevent hanging on bugs. Keep timeouts generous (5s) to avoid flaky tests.

Do NOT import testcontainers. Do NOT create real DB connections. Pure mock-based unit tests.
  </action>
  <verify>pytest tests/collector/test_daemon.py -v -x</verify>
  <done>All lifecycle tests pass: constructor, run starts tasks, stop cancels tasks, polling loop calls collect_once, polling loop survives exceptions.</done>
</task>

<task type="auto">
  <name>Task 2: Tests for crash recovery and health logging</name>
  <files>tests/collector/test_daemon.py</files>
  <action>
Add crash recovery and health tests to tests/collector/test_daemon.py:

**Crash recovery tests:**

`test_monitor_detects_crashed_task`:
- Create a daemon, set _running=True
- Add a mock task to self._tasks that is done (task.done()=True, task.cancelled()=False, task.exception()=RuntimeError("boom"))
- Store the collector and interval in _polling_collectors
- Run _monitor_tasks for one iteration (set _running=False after ~12s or mock asyncio.sleep)
- Verify the task was recreated (self._tasks[name] is a new task)
- Verify _restart_counts[name] == 1

`test_monitor_exponential_backoff`:
- Verify delay calculation: for restart_counts 0→5s, 1→10s, 2→20s, 3→40s, 4→60s (capped)
- Can test this by checking the sleep call with the right delay, or by testing the formula directly

`test_monitor_max_restarts_gives_up`:
- Set _restart_counts["metadata"] = 5 (at max)
- Add a crashed metadata task
- Run _monitor_tasks for one iteration
- Verify task was NOT recreated
- Verify CRITICAL log message was emitted (use caplog)

`test_monitor_skips_during_shutdown`:
- Set _running=False before running _monitor_tasks
- Verify no restarts attempted

`test_trade_listener_restart_creates_new_instance`:
- Add a crashed "trades" task
- Run _monitor_tasks for one iteration
- Verify TradeListener was re-instantiated (self._trade_listener is a new object)
- WHY: TradeListener has internal state (queue, health) that may be corrupted

**Health logging tests:**

`test_get_health_returns_correct_structure`:
- Set up daemon with known _collector_stats values
- Mock TradeListener.get_health() to return known values
- Call daemon.get_health()
- Verify returned dict has: uptime_seconds, tasks_alive, tasks_dead, total_restarts, collectors, trade_listener

`test_collector_stats_updated_by_polling_loop`:
- Run _run_polling_loop for 1-2 iterations with a mock collector returning 10
- Verify _collector_stats[name]["total_items"] >= 10
- Verify _collector_stats[name]["last_collect_ts"] is not None

`test_collector_stats_tracks_errors`:
- Run _run_polling_loop with a collector that raises
- Verify _collector_stats[name]["error_count"] >= 1
- Verify _collector_stats[name]["last_error"] is not None

Use pytest fixtures from Task 1 (same file). Use `caplog` fixture for log verification.
Do NOT test _health_log_loop directly — it's just formatting. Test the data it reads (get_health, _collector_stats).
  </action>
  <verify>pytest tests/collector/test_daemon.py -v</verify>
  <done>All tests pass: crash detection, exponential backoff, max restarts, TradeListener re-instantiation, get_health structure, stats tracking. Phase 4 complete.</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] `pytest tests/collector/test_daemon.py -v` — all tests pass
- [ ] No import errors or warnings
- [ ] Tests are pure unit tests (no DB, no network, no testcontainers)
- [ ] All daemon behaviors verified: lifecycle, crash recovery, health
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- All tests pass with no failures
- No new warnings introduced
- Phase 4 complete: daemon orchestrates collectors, handles signals, recovers from crashes, logs health, CLI `collect` command available
</success_criteria>

<output>
After completion, create `.planning/phases/04-daemon-supervisor-cli/04-03-SUMMARY.md`:

# Phase 04 Plan 03: Daemon Tests Summary

**[Substantive one-liner]**

## Accomplishments
- [Key outcomes]

## Files Created/Modified
- `tests/collector/test_daemon.py` - Description

## Decisions Made
[Key decisions]

## Issues Encountered
[Problems and resolutions]

## Phase 4 Completion
Phase 4: Daemon Supervisor + CLI is **COMPLETE**.
Ready for Phase 5: Hetzner Deployment.
</output>
