---
phase: 04-daemon-supervisor-cli
plan: 01
type: execute
---

<objective>
Create the CollectorDaemon class that orchestrates all 5 collectors as asyncio tasks with graceful shutdown, signal handling, and crash recovery.

Purpose: Single entry point to run 24/7 data collection — the daemon manages all collectors as concurrent tasks, handles OS signals for clean shutdown, and automatically restarts crashed collectors.
Output: `src/collector/daemon.py` with a fully functional daemon orchestrator.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Auto-selected based on dependency graph:
@.planning/phases/03-websocket-trades/03-04-SUMMARY.md
@.planning/phases/02-core-collectors/02-01-SUMMARY.md

# Key files:
@src/collector/market_metadata.py
@src/collector/price_snapshots.py
@src/collector/orderbook_snapshots.py
@src/collector/resolution_tracker.py
@src/collector/trade_listener.py
@src/config.py
@src/db/pool.py
@src/utils/heartbeat.py

**Tech stack available:** asyncpg, websockets, httpx, respx, pydantic, testcontainers
**Established patterns:** collect_once() -> int (never raises), run()/stop() lifecycle, TradeListenerHealth dataclass, pool singleton, CollectorConfig intervals

**Constraining decisions:**
- [02-01]: Collector pattern: __init__(pool, client, config), collect_once() -> int, never raises
- [03-04]: TradeListener has run()/stop() lifecycle (different from poll-based collectors)
- [03-04]: Token list fetched once at startup — daemon handles restarts for refresh
- [STATE]: Windows dev: asyncpg requires WindowsSelectorEventLoopPolicy
- [STATE]: py-clob-client is synchronous — all CLOB calls must use run_in_executor()
- [STATE]: ResolutionTracker and TradeListener do NOT take client param (only pool + config)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CollectorDaemon with run() and stop()</name>
  <files>src/collector/daemon.py</files>
  <action>
Create CollectorDaemon class in src/collector/daemon.py:

**Constructor** `__init__(self, pool: asyncpg.Pool, client: PolymarketClient, config: CollectorConfig)`:
- Store pool, client, config
- Instantiate all 5 collectors:
  - `self._metadata = MarketMetadataCollector(pool, client, config)`
  - `self._prices = PriceSnapshotCollector(pool, client, config)`
  - `self._orderbooks = OrderbookSnapshotCollector(pool, client, config)`
  - `self._resolutions = ResolutionTracker(pool, config)` (NO client param)
  - `self._trade_listener = TradeListener(pool, config)` (NO client param)
- Initialize: `self._running = False`, `self._tasks: dict[str, asyncio.Task] = {}`, `self._shutdown_event = asyncio.Event()`

**`_run_polling_loop(self, name: str, collector, interval_sec: int)`** — async method:
- Infinite loop while self._running:
  - Call `count = await collector.collect_once()`
  - `logger.debug("%s: collected %d items", name, count)`
  - `await asyncio.sleep(interval_sec)`
- Catch ALL exceptions except CancelledError — log with `logger.error("%s: collection error", name, exc_info=True)`, then `await asyncio.sleep(interval_sec)` and continue
- On CancelledError: return cleanly (do NOT catch and continue)
- IMPORTANT: Never re-raise from the except block — one collector failure must not crash the loop

**`run(self)`** — async method:
- Set `self._running = True`
- Register signal handlers for graceful shutdown:
  - Get the running loop: `loop = asyncio.get_running_loop()`
  - On non-Windows (`sys.platform != 'win32'`): use `loop.add_signal_handler(signal.SIGINT, self._shutdown_event.set)` and same for `signal.SIGTERM`
  - On Windows: use `signal.signal(signal.SIGINT, lambda s, f: self._shutdown_event.set())` — loop.add_signal_handler is NOT supported on Windows
- Start 4 polling tasks:
  - `self._tasks["metadata"] = asyncio.create_task(self._run_polling_loop("metadata", self._metadata, self.config.metadata_interval_sec))`
  - Same for "prices" (price_interval_sec), "orderbooks" (orderbook_interval_sec), "resolutions" (resolution_check_interval_sec)
- Start trade listener: `self._tasks["trades"] = asyncio.create_task(self._trade_listener.run())`
- `logger.info("Daemon started with %d tasks", len(self._tasks))`
- `await self._shutdown_event.wait()` — blocks until signal received
- `await self.stop()`

**`stop(self)`** — async method:
- If not self._running, return (idempotent)
- Set `self._running = False`
- `logger.info("Daemon shutting down...")`
- Cancel all polling tasks (metadata, prices, orderbooks, resolutions) via task.cancel()
- Call `await self._trade_listener.stop()` for graceful queue flush (do NOT just cancel the trade task)
- `await asyncio.gather(*self._tasks.values(), return_exceptions=True)`
- `logger.info("Daemon stopped")`

Import: `import asyncio, logging, signal, sys` and all 5 collector classes, PolymarketClient, CollectorConfig.
  </action>
  <verify>python -c "from src.collector.daemon import CollectorDaemon; print('import ok')"</verify>
  <done>CollectorDaemon class exists with __init__, _run_polling_loop, run, stop. Import succeeds. Signal handling works on both Windows and Linux.</done>
</task>

<task type="auto">
  <name>Task 2: Add crash recovery with auto-restart</name>
  <files>src/collector/daemon.py</files>
  <action>
Add crash recovery to CollectorDaemon:

**Add instance variables** in __init__:
- `self._restart_counts: dict[str, int] = {}` — track restarts per collector
- `self._max_restarts: int = 5` — stop retrying after 5 crashes
- `self._base_restart_delay: float = 5.0` — base delay in seconds
- `self._max_restart_delay: float = 60.0` — cap delay at 60s

**`_monitor_tasks(self)`** — async method, runs every 10 seconds:
- `while self._running:`
  - `await asyncio.sleep(10)`
  - For each name, task in self._tasks.items():
    - If `task.done() and not task.cancelled()`:
      - Get exception: `exc = task.exception()` (in try/except — task.exception() raises InvalidStateError if cancelled)
      - Log: `logger.error("Task '%s' crashed: %s", name, exc)`
      - Get restart count: `count = self._restart_counts.get(name, 0)`
      - If count >= self._max_restarts: `logger.critical("Task '%s' exceeded max restarts (%d), giving up", name, self._max_restarts)`, continue
      - Calculate delay: `delay = min(self._base_restart_delay * (2 ** count), self._max_restart_delay)`
      - Log: `logger.warning("Restarting '%s' in %.0fs (attempt %d/%d)", name, delay, count + 1, self._max_restarts)`
      - `await asyncio.sleep(delay)`
      - Check `self._running` again after sleep (may have shut down during delay)
      - Recreate the task:
        - For "trades": recreate TradeListener instance first (`self._trade_listener = TradeListener(self.pool, self.config)`) then `self._tasks[name] = asyncio.create_task(self._trade_listener.run())`
        - For polling tasks: recreate task from stored collector reference and interval
        - WHY recreate TradeListener: it has internal state (queue, health, tasks) that may be corrupted after crash
      - Increment: `self._restart_counts[name] = count + 1`
  - On CancelledError: return

**Store collector-interval mapping** for restart:
- Add `self._polling_collectors: dict[str, tuple[object, int]] = {}` in __init__ (populated after collector creation)
- Map: `{"metadata": (self._metadata, config.metadata_interval_sec), ...}`
- Use this mapping in _monitor_tasks to recreate polling tasks

**Start monitor in run()**:
- Add after starting collector tasks: `self._tasks["_monitor"] = asyncio.create_task(self._monitor_tasks())`
- Do NOT restart the monitor itself — if it crashes, log and continue

**In stop()**: cancel the monitor task along with everything else.
  </action>
  <verify>python -c "from src.collector.daemon import CollectorDaemon; print('crash recovery ok')"</verify>
  <done>_monitor_tasks() runs every 10s, detects crashed tasks, restarts with exponential backoff (5s→10s→20s→40s→60s cap). Max 5 restarts per collector. TradeListener gets fresh instance on restart.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from src.collector.daemon import CollectorDaemon"` succeeds
- [ ] CollectorDaemon has run(), stop(), _run_polling_loop(), _monitor_tasks()
- [ ] Signal handling is cross-platform (Windows: signal.signal, Linux: loop.add_signal_handler)
- [ ] No syntax errors or import failures
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No import errors
- CollectorDaemon orchestrates all 5 collectors with correct constructor signatures
- Crash recovery auto-restarts with exponential backoff
- Signal handling works on both Windows and Linux
</success_criteria>

<output>
After completion, create `.planning/phases/04-daemon-supervisor-cli/04-01-SUMMARY.md`
</output>
